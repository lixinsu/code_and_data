{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient 解决 Cart Pole V0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Car pole游戏目标</h3>\n",
    "在car（小车）上立pole（柱子）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Cart Pole V0 </h3>\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*G_whtIrY9fGlw3It6HFfhA.gif\" alt=\"Cart Pole game\" />\n",
    "\n",
    "4 种状态信息:\n",
    "<ul>\n",
    "    <li> 车的位置</li>\n",
    "    <li> 车的速度 </li>\n",
    "    <li> 柱子的位置 </li>\n",
    "    <li> 柱子的速度 </li>\n",
    "</ul>\n",
    "<br>\n",
    "agent的动作:\n",
    "<ul>\n",
    "    <li> 0: 向左 </li>\n",
    "    <li> 1: 向右 </ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资源\n",
    "<ul>\n",
    "    <li> <a href=\"https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724\">Simple Reinforcement Learning with Tensorflow: Part 2 - Policy-based Agents </a> </li>\n",
    "    \n",
    "   \n",
    "  <li> <a href=\"https://gist.github.com/shanest/535acf4c62ee2a71da498281c2dfc4f4\" >Policy gradients for reinforcement learning in TensorFlow</a></li>\n",
    "  </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入依赖的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lixin/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/lixin/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/lixin/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/lixin/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/lixin/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/lixin/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lixin/anaconda3/envs/py36/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# 先看下agent随机策略的效果\n",
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    \n",
    "    # 采用随机动作，左右采样一个动作\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指定超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4 # state的种类\n",
    "action_size = 2 # 2 actions 种类\n",
    "hidden_size = 64 # 隐层单元数\n",
    "\n",
    "learning_rate = 0.001 \n",
    "gamma = 0.99 # 打折比例\n",
    "\n",
    "train_episodes = 5000 # 游戏轮数\n",
    "max_steps = 900 # 最大步数\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGAgent():\n",
    "    def __init__(self, input_size, action_size, hidden_size, learning_rate, gamma):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # 构建网络\n",
    "        self.inputs = tf.placeholder(tf.float32, \n",
    "                      shape = [None, input_size])\n",
    "                              \n",
    "        self.hidden_layer_1 = tf.contrib.layers.fully_connected(inputs = self.inputs,\n",
    "                                                  num_outputs = hidden_size,\n",
    "                                                  activation_fn = tf.nn.elu,\n",
    "                                                  weights_initializer = tf.random_normal_initializer())\n",
    "\n",
    "        self.output_layer = tf.contrib.layers.fully_connected(inputs = self.hidden_layer_1,\n",
    "                                                         num_outputs = action_size,\n",
    "                                                 activation_fn = tf.nn.softmax)\n",
    "        \n",
    "        # Log prob output\n",
    "        self.output_log_prob = tf.log(self.output_layer)\n",
    "        \n",
    "        \n",
    "        ### 损失函数 : 把reward 和 chosen action 输入 DNN\n",
    "        # 参考实现 https://gist.github.com/shanest/535acf4c62ee2a71da498281c2dfc4f4\n",
    "        \n",
    "        self.actions = tf.placeholder(tf.int32, shape = [None])\n",
    "        self.rewards = tf.placeholder(tf.float32, shape = [None])\n",
    "        \n",
    "        # 获得 log probability of actions from episode : \n",
    "        self.indices = tf.range(0, tf.shape(self.output_log_prob)[0]) * tf.shape(self.output_log_prob)[1] + self.actions\n",
    "        \n",
    "        self.actions_probability = tf.gather(tf.reshape(self.output_layer, [-1]), self.indices)\n",
    "        \n",
    "        self.loss = -tf.reduce_mean(tf.log(self.actions_probability) * self.rewards)\n",
    "        \n",
    "  \n",
    "\n",
    "        # 收集梯度 after some training episodes outside the graph and then apply them.\n",
    "        # 参考 https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.mtwpvfi8b\n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for idx,var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32, name=str(idx)+ '_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss,tvars)\n",
    "        \n",
    "        \n",
    "        ### 优化器\n",
    "        \n",
    "        #  RMSProp效果较好\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义优势函数   \n",
    "通过该优势函数告诉代理怎么做正确的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>早期的reward优于长期reward，对长期reward进行打折.</b>\n",
    "</p>\n",
    "<img src=\"assets/discountreward.png\" alt=\"Discount reward\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "延迟的reward影响更小：举个例子5步之后柱子已经非常倾斜，那之后的reward无意义，因为柱子已经无法修正"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/d1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/d2.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对即时reward的和延迟reward应用不同的权重\n",
    "\n",
    "def discount_rewards(r):\n",
    "    # 初始化reward打折矩阵\n",
    "    discounted_reward= np.zeros_like(r) \n",
    "    \n",
    "    # 存储reward 的和\n",
    "    running_add = 0\n",
    "    \n",
    "    # 遍历rewards\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        \n",
    "        running_add = running_add * gamma + r[t] # sum * y (gamma) + reward\n",
    "        discounted_reward[t] = running_add\n",
    "    return discounted_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/lixin/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/lixin/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Episode: 0 Total reward: 14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lixin/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 Total reward: 13.24\n",
      "Episode: 200 Total reward: 12.92\n",
      "Episode: 300 Total reward: 13.59\n",
      "Episode: 400 Total reward: 13.81\n",
      "Episode: 500 Total reward: 14.44\n",
      "Episode: 600 Total reward: 16.27\n",
      "Episode: 700 Total reward: 19.48\n",
      "Episode: 800 Total reward: 37.92\n",
      "Episode: 900 Total reward: 59.65\n",
      "Episode: 1000 Total reward: 110.99\n"
     ]
    }
   ],
   "source": [
    "# 清除图\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "agent = PGAgent(input_size, action_size, hidden_size, learning_rate, gamma)\n",
    "\n",
    "# 定义tf图\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    nb_episodes = 0\n",
    "    \n",
    "    # 定义 total_rewards 和 total_length\n",
    "    total_reward = []\n",
    "    total_length = []\n",
    "    \n",
    " \n",
    "    gradBuffer = sess.run(tf.trainable_variables())\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "        \n",
    "    \n",
    "    # While we have episodes to train\n",
    "    while nb_episodes < train_episodes:\n",
    "        state = env.reset()\n",
    "        running_reward = 0\n",
    "        episode_history = [] # Init the array that keep track the history in an episode\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            #Probabilistically pick an action given our network outputs.\n",
    "            # Not my implementation: taken from Udacity Q-learning quart https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb \n",
    "            action_distribution = sess.run(agent.output_layer ,feed_dict={agent.inputs:[state]})\n",
    "            action = np.random.choice(action_distribution[0],p=action_distribution[0])\n",
    "            action = np.argmax(action_distribution == action)\n",
    "            \n",
    "            state_1, reward, done, info = env.step(action)\n",
    "            \n",
    "            # 把当前步加入到 episode历史\n",
    "            episode_history.append([state, action, reward, state_1])\n",
    "            \n",
    "            #  state 现在为 state 1\n",
    "            state = state_1\n",
    "            \n",
    "            running_reward += reward\n",
    "            \n",
    "            if done == True:\n",
    "                # 更新网络参数\n",
    "                episode_history = np.array(episode_history)\n",
    "                episode_history[:,2] = discount_rewards(episode_history[:,2])\n",
    "                feed_dict={agent.rewards:episode_history[:,2],\n",
    "                        agent.actions:episode_history[:,1],agent.inputs:np.vstack(episode_history[:,0])}\n",
    "                grads = sess.run(agent.gradients, feed_dict=feed_dict)\n",
    "                \n",
    "                \n",
    "                for idx,grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "\n",
    "                if nb_episodes % batch_size == 0 and nb_episodes != 0:\n",
    "                    feed_dict= dictionary = dict(zip(agent.gradient_holders, gradBuffer))\n",
    "                    _ = sess.run(agent.update_batch, feed_dict=feed_dict)\n",
    "                    for ix,grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                #(running_reward))\n",
    "                total_reward.append(running_reward)\n",
    "                total_length.append(step)\n",
    "                break\n",
    "                \n",
    "        # 每 100 episodes 打印\n",
    "        if nb_episodes % 100 == 0:\n",
    "            print(\"Episode: {}\".format(nb_episodes),\n",
    "                    \"Total reward: {}\".format(np.mean(total_reward[-100:])))\n",
    "        nb_episodes += 1\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/cartPoleGame.ckpt\")\n",
    "        \n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 测试agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f9c86dc4beeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'saver' is not defined"
     ]
    }
   ],
   "source": [
    "test_episodes = 10\n",
    "test_max_steps = 400\n",
    "env.reset()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    for episode in range(1, test_episodes):\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            env.render() \n",
    "            \n",
    "        \n",
    "            \n",
    "            #从输出中概率性选择action.\n",
    "            # 参考 https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb \n",
    "            action_distribution = sess.run(agent.output_layer ,feed_dict={agent.inputs:[state]})\n",
    "            action = np.random.choice(action_distribution[0],p=action_distribution[0])\n",
    "            action = np.argmax(action_distribution == action)\n",
    "            \n",
    "            state_1, reward, done, info = env.step(action)\n",
    "           \n",
    "            \n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                # 采样动作\n",
    "                state, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                state = state_1 # Next state\n",
    "                t += 1\n",
    "                \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
